---
title: "Chapter 3, Example 3: Regression"
output: html_notebook
---

# Predicting House Prices

Use Boston dataset for a regression example.

```{r}
library(keras)
dataset <- dataset_boston_housing()
c(c(train_data, train_targets), c(test_data, test_targets)) %<-% dataset
str(train_data)
str(test_data)
str(train_targets)
```

Normalise each variable to have mean zero and unit variance. (NB. use the mean and standard deviation from training for normalising all data.)

```{r}
means <- colMeans(train_data)
std_devs <- apply(train_data, 2, sd)
train_data <- scale(train_data, center = means, scale = std_devs)
test_data <- scale(test_data, center = means, scale = std_devs)
```

Create a function that fits the model: it will be called repeatedly for cross-validation.

```{r}
build_model <- function() {
    model <- keras_model_sequential() %>%
        layer_dense(units = 64, activation = "relu",
                    input_shape = dim(train_data)[[2]]) %>%
        layer_dense(units = 64, activation = "relu") %>%
        layer_dense(units = 1)
    model %>% 
        compile(
            optimizer = "rmsprop",
            loss = "mse",
            metrics = c("mae"))
}
```

Then do cross-validation via a for loop. (NB. maybe try redoing this with a `purrr` function?)

```{r}
k <- 4
indices <- sample(seq_len(nrow(train_data)))
folds <- cut(indices, breaks = k, labels = FALSE)
num_epochs <- 100
all_scores <- c()
for (i in 1:k) {
    cat("processing fold #", i, "\n")
    val_indices <- which(folds == i, arr.ind = TRUE)
    val_data <- train_data[val_indices,]
    val_targets <- train_targets[val_indices]
    partial_train_data <- train_data[-val_indices,]
    partial_train_targets <- train_targets[-val_indices]
    model <- build_model()
    model %>% fit(partial_train_data, partial_train_targets,
                  epochs = num_epochs, batch_size = 1, verbose = 0)
    results <- model %>% evaluate(val_data, val_targets, verbose = 0)
    all_scores <- c(all_scores, results$mean_absolute_error)
}
```

Check scores across folds.

```{r}
all_scores
mean(all_scores)
```

Try the same again but for 500 epochs, and save the results for use afterwards.

```{r}
num_epochs <- 500
all_mae_histories <- NULL
for (i in 1:k) {
    cat("processing fold #", i, "\n")
    val_indices <- which(folds == i, arr.ind = TRUE)
    val_data <- train_data[val_indices,]
    val_targets <- train_targets[val_indices]
    partial_train_data <- train_data[-val_indices,]
    partial_train_targets <- train_targets[-val_indices]
    model <- build_model()
    history <- model %>% fit(
        partial_train_data, partial_train_targets,
        validation_data = list(val_data, val_targets),
        epochs = num_epochs, batch_size = 1, verbose = 0)
    mae_history <- history$metrics$val_mean_absolute_error
    all_mae_histories <- rbind(all_mae_histories, mae_history)
}
```

```{r}
average_mae_history <- data.frame(
    epoch = seq(1:ncol(all_mae_histories)),
    validation_mae = apply(all_mae_histories, 2, mean))
```

```{r}
library(ggplot2)
ggplot(average_mae_history, aes(epoch, validation_mae)) + 
    geom_line()
```

```{r}
ggplot(average_mae_history, aes(epoch, validation_mae)) + 
    geom_smooth()
```

Past 125 epochs there's no gain, start to see overfitting.

```{r}
model <- build_model()
model %>% fit(train_data, train_targets,
              epochs = 80, batch_size = 16, verbose = 0)
result <- model %>% 
    evaluate(test_data, test_targets)
result
```



